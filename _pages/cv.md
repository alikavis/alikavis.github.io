---
layout: archive
title: ""
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Research Interests
======
convex/non-convex optimization, stochastic methods, adaptive algorithms, online learning, machine learning

Education
======
* B.S. in Computer Engineering (Salutatorian), Bilkent University. 2012-2017
* Ph.D in Computer and Communication Sciences, EPFL. 09/2017 - 08/2023
* Postdoc at IFML, UT Austin. 11/2023 - Present
  
Employment
======
* Postdoctoral Fellow at University of Texas at Austin. 11/2023 - Preent
  * Department of Electrical and Computer Engineering
  * Supervisors: Sujay Sanghavi and Aryan Mokhtari

* Research Assistant at EPFL, Switzerland. 09/2017 - 08/2023
  * School of Computer and Communication Sciences
  * Supervisor: Volkan Cevher
  
Honors and Awards
======
* Swiss National Science Foundation Postdoc.Mobility Return Grant (CHF 115K), 2026-2027
* Swiss National Science Foundation Postdoc.Mobility Grant (CHF 120K), 2023-2025
* EPFL IC Doctoral Studies Fellowship, 2017-2018
* Spotlight Paper at NeurIPS 2018, 2019, 2023 and ICML 2025
* Top Reviewer at NeurIPS 2024 and ICML 2025
* Travel Award at Neurips, 2019
* Comprehensive Scholarship (Tuition waiver with monthly stipend), Bilkent University, 2012-2017
  
Professional Service
======
* Reviewer Duty : NeurIPS, ICML, COLT, ICLR, OJMO, JMLR, Springer Machine Learning, IEEE Transactions on Information Theory

* Tutorial at EUSIPCO 2021. 01/2021
  * Adaptive Optimization Methods for Machine Learning and Signal Processing
  * Lecturers: Ali Kavis, Ahmet Alacaoglu, Kfir Levy, Volkan Cevher
 
* Minisymposium at SIAM OP23. 05/2023
  * Adaptivity and Universality: First-order Methods and Beyond
  * Organizers: Volkan Cevher, Ali Kavis, Kimon Antonakopoulos
 
Invited Talks
======
* INFORMS 2025 (at Atlanta). *Parameter-free second-order methods for min-max optimization.*
* ICCOPT 2025 (at USC). *Parameter-free second-order methods for min-max optimization.*
* EUROPT 2025 (at University of Southampton). *Parameter-free second-order methods for min-max optimization.*
* IFML Symposium Workshop (at Simons Institute) 2024. *Parameter-free second-order methods for min-max optimization.*
* ODI Group - Niao He (at ETH Zurich) 2024. *Parameter-free second-order methods for min-max optimization.*
* IFML Seminar (at UT Austin) 2024. *Designing fast, parameter-free algorithms for convex optimization.*
* SIAM Conference on Optimization 2023. *A first approach to noise-adaptive accelerated second-order methods.*
* SIAM MDS Conference 2022. *High probability convergence of AdaGrad for non-convex optimization.*
 
Publications
======
  <ul>{% for post in site.publications reversed %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul>
<!-- Talks
======
  <ul>{% for post in site.talks %}
    {% include archive-single-talk-cv.html %}
  {% endfor %}</ul> -->


